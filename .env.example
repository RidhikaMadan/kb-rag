# ============================================
# KB-RAG Environment Variables
# ============================================
# Copy this file to .env and update with your values
# cp .env.example .env

# ============================================
# OpenAI Configuration
# ============================================
# Your OpenAI API key (required if not using local model)
# Get one at: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# ============================================
# Local LLM Configuration
# ============================================
# Set to "true" to use local Llama model instead of OpenAI
# Requires model file at: models/Llama-3.2-3B-Instruct-Q4_K_M.gguf
USE_LOCAL_LLM=false

# ============================================
# MongoDB Configuration
# ============================================
# MongoDB connection string
# Default: mongodb://localhost:27017/
# For Docker: mongodb://mongodb:27017/
# For MongoDB Atlas: mongodb+srv://username:password@cluster.mongodb.net/
MONGODB_URI=mongodb://localhost:27017/

# MongoDB database name
# Default: rag_chatbot
MONGODB_DB=rag_chatbot

# ============================================
# Knowledge Base Configuration
# ============================================
# Path to knowledge base folder
# Default: KB
KB_FOLDER=KB

# ============================================
# Frontend Configuration
# ============================================
# Backend API URL for frontend
# Default (dev): http://localhost:8000
# Default (prod): empty string (uses relative URLs)
# VITE_API_URL=http://localhost:8000

# ============================================
# Optional: Performance Settings
# ============================================
# Disable tokenizers parallelism warnings
# Set to "false" to avoid warnings
TOKENIZERS_PARALLELISM=false
